{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\"))\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGDataLoader:\n",
    "    def __init__(self):\n",
    "        self.cfg = {\n",
    "            'item2id_path': 'data/item_index2entity_id.txt',\n",
    "            'kg_path': 'data/kg.txt',\n",
    "            'rating_path': 'data/ratings.csv',\n",
    "            'rating_sep': ',',\n",
    "            'threshold': 4.0\n",
    "        }\n",
    "\n",
    "        # Load files\n",
    "        df_item2id = pd.read_csv(self.cfg['item2id_path'], sep='\\t', header=None, names=['item', 'id'])\n",
    "        df_kg = pd.read_csv(self.cfg['kg_path'], sep='\\t', header=None, names=['head', 'relation', 'tail'])\n",
    "        df_rating = pd.read_csv(self.cfg['rating_path'], sep=self.cfg['rating_sep'], \n",
    "                                names=['userID', 'itemID', 'rating'], skiprows=1)\n",
    "\n",
    "        # Filter only items that exist in item2id\n",
    "        df_rating = df_rating[df_rating['itemID'].isin(df_item2id['item'])]\n",
    "        df_rating.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        self.df_item2id = df_item2id\n",
    "        self.df_kg = df_kg\n",
    "        self.df_rating = df_rating\n",
    "\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.entity_encoder = LabelEncoder()\n",
    "        self.relation_encoder = LabelEncoder()\n",
    "\n",
    "        self._encoding()\n",
    "\n",
    "    def _encoding(self):\n",
    "        self.user_encoder.fit(self.df_rating['userID'])\n",
    "        self.entity_encoder.fit(pd.concat([self.df_item2id['id'], self.df_kg['head'], self.df_kg['tail']]))\n",
    "        self.relation_encoder.fit(self.df_kg['relation'])\n",
    "\n",
    "        self.df_kg['head'] = self.entity_encoder.transform(self.df_kg['head'])\n",
    "        self.df_kg['tail'] = self.entity_encoder.transform(self.df_kg['tail'])\n",
    "        self.df_kg['relation'] = self.relation_encoder.transform(self.df_kg['relation'])\n",
    "\n",
    "    def _build_dataset(self):\n",
    "        print('Build dataset dataframe ...', end=' ')\n",
    "        df_dataset = pd.DataFrame()\n",
    "        df_dataset['userID'] = self.user_encoder.transform(self.df_rating['userID'])\n",
    "\n",
    "        item2id_dict = dict(zip(self.df_item2id['item'], self.df_item2id['id']))\n",
    "        self.df_rating['itemID'] = self.df_rating['itemID'].apply(lambda x: item2id_dict[x])\n",
    "        df_dataset['itemID'] = self.entity_encoder.transform(self.df_rating['itemID'])\n",
    "        df_dataset['label'] = self.df_rating['rating'].apply(lambda x: 0 if x < self.cfg['threshold'] else 1)\n",
    "\n",
    "        df_dataset = df_dataset[df_dataset['label'] == 1]\n",
    "\n",
    "        full_item_set = set(range(len(self.entity_encoder.classes_)))\n",
    "        user_list, item_list, label_list = [], [], []\n",
    "        for user, group in df_dataset.groupby('userID').__iter__():\n",
    "            item_set = set(group['itemID'])\n",
    "            negative_set = full_item_set - item_set\n",
    "            negative_sampled = random.sample(list(negative_set), len(item_set))\n",
    "            user_list.extend([user] * len(negative_sampled))\n",
    "            item_list.extend(negative_sampled)\n",
    "            label_list.extend([0] * len(negative_sampled))\n",
    "\n",
    "        negative = pd.DataFrame({'userID': user_list, 'itemID': item_list, 'label': label_list})\n",
    "        df_dataset = pd.concat([df_dataset, negative])\n",
    "\n",
    "        df_dataset = df_dataset.sample(frac=1, replace=False, random_state=999)\n",
    "        df_dataset.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        print('Done')\n",
    "        return df_dataset\n",
    "\n",
    "    def _construct_kg(self):\n",
    "        print('Construct knowledge graph ...', end=' ')\n",
    "        kg = dict()\n",
    "        for i in range(len(self.df_kg)):\n",
    "            head = self.df_kg.iloc[i]['head']\n",
    "            relation = self.df_kg.iloc[i]['relation']\n",
    "            tail = self.df_kg.iloc[i]['tail']\n",
    "            if head in kg:\n",
    "                kg[head].append((relation, tail))\n",
    "            else:\n",
    "                kg[head] = [(relation, tail)]\n",
    "            if tail in kg:\n",
    "                kg[tail].append((relation, head))\n",
    "            else:\n",
    "                kg[tail] = [(relation, head)]\n",
    "\n",
    "        print('Done')\n",
    "        return kg\n",
    "\n",
    "    def load_dataset(self):\n",
    "        return self._build_dataset()\n",
    "\n",
    "    def load_kg(self):\n",
    "        return self._construct_kg()\n",
    "\n",
    "    def get_encoders(self):\n",
    "        return (self.user_encoder, self.entity_encoder, self.relation_encoder)\n",
    "\n",
    "    def get_num(self):\n",
    "        return (len(self.user_encoder.classes_), len(self.entity_encoder.classes_), len(self.relation_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build dataset dataframe ... Done\n",
      "userID    int64\n",
      "itemID    int64\n",
      "label     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "loader = KGDataLoader()\n",
    "\n",
    "df_dataset = loader.load_dataset()\n",
    "\n",
    "print(df_dataset.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, dim, aggregator):\n",
    "        super(Aggregator, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.aggregator = aggregator\n",
    "        \n",
    "        if aggregator == 'concat':\n",
    "            self.weights = torch.nn.Linear(2 * dim, dim, bias=True)\n",
    "        else:\n",
    "            self.weights = torch.nn.Linear(dim, dim, bias=True)\n",
    "        \n",
    "    def forward(self, self_vectors, neighbor_vectors, neighbor_relations, user_embeddings, act):\n",
    "        batch_size = user_embeddings.size(0)\n",
    "        if batch_size != self.batch_size:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "        neighbors_agg = self._mix_neighbor_vectors(neighbor_vectors, neighbor_relations, user_embeddings)\n",
    "\n",
    "        if self.aggregator == 'sum':\n",
    "            output = self_vectors + neighbors_agg\n",
    "        elif self.aggregator == 'concat':\n",
    "            output = torch.cat((self_vectors, neighbors_agg), dim=-1)\n",
    "        else:  # only neighbors\n",
    "            output = neighbors_agg\n",
    "\n",
    "        output = output.view(-1, output.shape[-1])  # [batch_size * hop_len, dim or 2*dim]\n",
    "        output = self.weights(output)\n",
    "        return act(output.view(self.batch_size, -1, self.dim))\n",
    "        \n",
    "    def _mix_neighbor_vectors(self, neighbor_vectors, neighbor_relations, user_embeddings):\n",
    "        batch_size, n_total, dim = neighbor_vectors.shape  # e.g., [512, 4096, 32]\n",
    "\n",
    "        user_embeddings = user_embeddings.view(batch_size, 1, dim).expand(batch_size, n_total, dim)\n",
    "\n",
    "        user_relation_scores = (user_embeddings * neighbor_relations).sum(dim=-1)  # [batch_size, n_total]\n",
    "        user_relation_scores_normalized = F.softmax(user_relation_scores, dim=-1)  # [batch_size, n_total]\n",
    "        user_relation_scores_normalized = user_relation_scores_normalized.unsqueeze(dim=-1)  # [batch_size, n_total, 1]\n",
    "\n",
    "        neighbors_aggregated = (user_relation_scores_normalized * neighbor_vectors).sum(dim=1)  # [batch_size, dim]\n",
    "        return neighbors_aggregated.unsqueeze(1)  # [batch_size, 1, dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGCN(torch.nn.Module):\n",
    "    def __init__(self, num_user, num_ent, num_rel, kg, args, device):\n",
    "        super(KGCN, self).__init__()\n",
    "        self.num_user = num_user\n",
    "        self.num_ent = num_ent\n",
    "        self.num_rel = num_rel\n",
    "        self.n_iter = args.n_iter\n",
    "        self.batch_size = args.batch_size\n",
    "        self.dim = args.dim\n",
    "        self.n_neighbor = args.neighbor_sample_size\n",
    "        self.kg = kg\n",
    "        self.device = device\n",
    "        self.aggregator = Aggregator(self.batch_size, self.dim, args.aggregator)\n",
    "        \n",
    "        self._gen_adj()\n",
    "            \n",
    "        self.usr = torch.nn.Embedding(num_user, args.dim)\n",
    "        self.ent = torch.nn.Embedding(num_ent, args.dim)\n",
    "        self.rel = torch.nn.Embedding(num_rel, args.dim)\n",
    "        \n",
    "    def _gen_adj(self):\n",
    "        self.adj_ent = torch.empty(self.num_ent, self.n_neighbor, dtype=torch.long)\n",
    "        self.adj_rel = torch.empty(self.num_ent, self.n_neighbor, dtype=torch.long)\n",
    "        \n",
    "        for e in self.kg:\n",
    "            if len(self.kg[e]) >= self.n_neighbor:\n",
    "                neighbors = random.sample(self.kg[e], self.n_neighbor)\n",
    "            else:\n",
    "                neighbors = random.choices(self.kg[e], k=self.n_neighbor)\n",
    "                \n",
    "            self.adj_ent[e] = torch.LongTensor([ent for _, ent in neighbors])\n",
    "            self.adj_rel[e] = torch.LongTensor([rel for rel, _ in neighbors])\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        batch_size = u.size(0)\n",
    "        if batch_size != self.batch_size:\n",
    "            self.batch_size = batch_size\n",
    "        # change to [batch_size, 1]\n",
    "        u = u.view((-1, 1))\n",
    "        v = v.view((-1, 1))\n",
    "        \n",
    "        # [batch_size, dim]\n",
    "        user_embeddings = self.usr(u).squeeze(dim = 1)\n",
    "        \n",
    "        entities, relations = self._get_neighbors(v)\n",
    "        \n",
    "        item_embeddings = self._aggregate(user_embeddings, entities, relations)\n",
    "        \n",
    "        scores = (user_embeddings * item_embeddings).sum(dim = 1)\n",
    "            \n",
    "        return torch.sigmoid(scores)\n",
    "    \n",
    "    def _get_neighbors(self, v):\n",
    "        entities = [v]\n",
    "        relations = []\n",
    "\n",
    "        for h in range(self.n_iter):\n",
    "            indices = entities[h].cpu()  # 💡 반드시 CPU 텐서로 바꿔줘야 함\n",
    "\n",
    "            neighbor_entities = self.adj_ent[indices].to(self.device)\n",
    "            neighbor_relations = self.adj_rel[indices].to(self.device)\n",
    "\n",
    "            entities.append(neighbor_entities)\n",
    "            relations.append(neighbor_relations)\n",
    "\n",
    "        return entities, relations\n",
    "    \n",
    "    def _aggregate(self, user_embeddings, entities, relations):\n",
    "        entity_vectors = [self.ent(entity) for entity in entities]\n",
    "        relation_vectors = [self.rel(relation) for relation in relations]\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            act = torch.tanh if i == self.n_iter - 1 else torch.sigmoid\n",
    "            entity_vectors_next_iter = []\n",
    "\n",
    "            for hop in range(self.n_iter - i):\n",
    "                # flatten neighbor vector (remove view error risk)\n",
    "                batch_size = entity_vectors[hop].shape[0]\n",
    "                hop_len = entity_vectors[hop].shape[1] if entity_vectors[hop].dim() > 1 else 1\n",
    "\n",
    "                self_vec = entity_vectors[hop]\n",
    "                neigh_vec = entity_vectors[hop + 1].reshape(self.batch_size, -1, self.dim)\n",
    "                neigh_rel = relation_vectors[hop].reshape(self.batch_size, -1, self.dim)\n",
    "\n",
    "                vector = self.aggregator(\n",
    "                    self_vectors=self_vec,\n",
    "                    neighbor_vectors=neigh_vec,\n",
    "                    neighbor_relations=neigh_rel,\n",
    "                    user_embeddings=user_embeddings,\n",
    "                    act=act,\n",
    "                )\n",
    "                entity_vectors_next_iter.append(vector)\n",
    "\n",
    "            entity_vectors = entity_vectors_next_iter\n",
    "\n",
    "        # 마지막 embedding은 [batch_size, dim] 형태로 반환\n",
    "        return entity_vectors[0].mean(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct knowledge graph ... Done\n",
      "Build dataset dataframe ... Done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>602</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>314</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8404</td>\n",
       "      <td>98598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>462</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12826</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14947307</th>\n",
       "      <td>7348</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14947308</th>\n",
       "      <td>721</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14947309</th>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14947310</th>\n",
       "      <td>249</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14947311</th>\n",
       "      <td>580</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14947312 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          userID  itemID  label\n",
       "0            602       3      1\n",
       "1            314       2      1\n",
       "2           8404   98598      0\n",
       "3            462       3      1\n",
       "4          12826       2      1\n",
       "...          ...     ...    ...\n",
       "14947307    7348       4      1\n",
       "14947308     721       0      1\n",
       "14947309      90       2      1\n",
       "14947310     249       2      1\n",
       "14947311     580       2      1\n",
       "\n",
       "[14947312 rows x 3 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dataset and knowledge graph\n",
    "data_loader = KGDataLoader()\n",
    "kg = data_loader.load_kg()\n",
    "df_dataset = data_loader.load_dataset()\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class KGCNDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        user_id = np.array(self.df.iloc[idx]['userID'])\n",
    "        item_id = np.array(self.df.iloc[idx]['itemID'])\n",
    "        label = np.array(self.df.iloc[idx]['label'], dtype=np.float32)\n",
    "        return user_id, item_id, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare arguments (hyperparameters)\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--aggregator', type=str, default='sum', help='which aggregator to use')\n",
    "parser.add_argument('--n_epochs', type=int, default=20, help='the number of epochs')\n",
    "parser.add_argument('--neighbor_sample_size', type=int, default=8, help='the number of neighbors to be sampled')\n",
    "parser.add_argument('--dim', type=int, default=32, help='dimension of user and entity embeddings')\n",
    "parser.add_argument('--n_iter', type=int, default=2, help='number of iterations when computing entity representation')\n",
    "parser.add_argument('--batch_size', type=int, default=512, help='batch size')\n",
    "parser.add_argument('--l2_weight', type=float, default=1e-4, help='weight of l2 regularization')\n",
    "parser.add_argument('--lr', type=float, default=5e-4, help='learning rate')\n",
    "parser.add_argument('--ratio', type=float, default=0.8, help='size of training dataset')\n",
    "\n",
    "args = parser.parse_args(['--l2_weight', '1e-4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct knowledge graph ... Done\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 구성\n",
    "train_data = TensorDataset(\n",
    "    torch.LongTensor(df_dataset['userID'].values),\n",
    "    torch.LongTensor(df_dataset['itemID'].values),\n",
    "    torch.FloatTensor(df_dataset['label'].values)\n",
    ")\n",
    "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "n_user, n_entity, n_relation = loader.get_num()\n",
    "\n",
    "kg_dict = loader.load_kg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_dataset, df_dataset['label'], test_size=1 - args.ratio, shuffle=False, random_state=999)\n",
    "train_dataset = KGCNDataset(x_train)\n",
    "test_dataset = KGCNDataset(x_test)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  mps\n"
     ]
    }
   ],
   "source": [
    "# prepare network, loss function, optimizer\n",
    "num_user, num_entity, num_relation = data_loader.get_num()\n",
    "user_encoder, entity_encoder, relation_encoder = data_loader.get_encoders()\n",
    "net = KGCN(num_user, num_entity, num_relation, kg, args, device).to(device)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2_weight)\n",
    "print('device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (4096) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[189], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m user_ids, item_ids, labels \u001b[38;5;241m=\u001b[39m user_ids\u001b[38;5;241m.\u001b[39mto(device), item_ids\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[182], line 47\u001b[0m, in \u001b[0;36mKGCN.forward\u001b[0;34m(self, u, v)\u001b[0m\n\u001b[1;32m     43\u001b[0m user_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musr(u)\u001b[38;5;241m.\u001b[39msqueeze(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     45\u001b[0m entities, relations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_neighbors(v)\n\u001b[0;32m---> 47\u001b[0m item_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m scores \u001b[38;5;241m=\u001b[39m (user_embeddings \u001b[38;5;241m*\u001b[39m item_embeddings)\u001b[38;5;241m.\u001b[39msum(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(scores)\n",
      "Cell \u001b[0;32mIn[182], line 85\u001b[0m, in \u001b[0;36mKGCN._aggregate\u001b[0;34m(self, user_embeddings, entities, relations)\u001b[0m\n\u001b[1;32m     82\u001b[0m     neigh_vec \u001b[38;5;241m=\u001b[39m entity_vectors[hop \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[1;32m     83\u001b[0m     neigh_rel \u001b[38;5;241m=\u001b[39m relation_vectors[hop]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[0;32m---> 85\u001b[0m     vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mself_vectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_vec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneighbor_vectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneigh_vec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneighbor_relations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneigh_rel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     entity_vectors_next_iter\u001b[38;5;241m.\u001b[39mappend(vector)\n\u001b[1;32m     94\u001b[0m entity_vectors \u001b[38;5;241m=\u001b[39m entity_vectors_next_iter\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[181], line 19\u001b[0m, in \u001b[0;36mAggregator.forward\u001b[0;34m(self, self_vectors, neighbor_vectors, neighbor_relations, user_embeddings, act)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m---> 19\u001b[0m neighbors_agg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mix_neighbor_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbor_relations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregator \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     22\u001b[0m     output \u001b[38;5;241m=\u001b[39m self_vectors \u001b[38;5;241m+\u001b[39m neighbors_agg\n",
      "Cell \u001b[0;32mIn[181], line 44\u001b[0m, in \u001b[0;36mAggregator._mix_neighbor_vectors\u001b[0;34m(self, neighbor_vectors, neighbor_relations, user_embeddings)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# [batch_size, n_total, 1] * [batch_size, n_total, dim] → [batch_size, dim]\u001b[39;00m\n\u001b[1;32m     43\u001b[0m user_relation_scores_normalized \u001b[38;5;241m=\u001b[39m user_relation_scores_normalized\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m neighbors_aggregated \u001b[38;5;241m=\u001b[39m (\u001b[43muser_relation_scores_normalized\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mneighbor_vectors\u001b[49m)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m neighbors_aggregated\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (4096) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# train\n",
    "loss_list = []\n",
    "test_loss_list = []\n",
    "auc_score_list = []\n",
    "\n",
    "for epoch in range(args.n_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (user_ids, item_ids, labels) in enumerate(train_loader):\n",
    "        user_ids, item_ids, labels = user_ids.to(device), item_ids.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(user_ids, item_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # print train loss per every epoch\n",
    "    print('[Epoch {}]train_loss: '.format(epoch+1), running_loss / len(train_loader))\n",
    "    loss_list.append(running_loss / len(train_loader))\n",
    "        \n",
    "    # evaluate per every epoch\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        total_roc = 0\n",
    "        for user_ids, item_ids, labels in test_loader:\n",
    "            user_ids, item_ids, labels = user_ids.to(device), item_ids.to(device), labels.to(device)\n",
    "            outputs = net(user_ids, item_ids)\n",
    "            test_loss += criterion(outputs, labels).item()\n",
    "            total_roc += roc_auc_score(labels.cpu().detach().numpy(), outputs.cpu().detach().numpy())\n",
    "        print('[Epoch {}]test_loss: '.format(epoch+1), test_loss / len(test_loader))\n",
    "        test_loss_list.append(test_loss / len(test_loader))\n",
    "        auc_score_list.append(total_roc / len(test_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
