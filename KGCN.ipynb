{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KGCN 논문 리뷰 & 코드 작성\n",
    "**KGCN: Simplifying and Powering Graph Convolution Network for Recommendation**  \n",
    "*Hongwei Wang et al. (2019)*  \n",
    "🔗 [논문 링크](https://arxiv.org/abs/1904.12575)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Problem Formulation\n",
    "\n",
    "*목표 : 사용자 u가 아이템 v에 관심 있을지를 예측하는 함수*\n",
    "$$\\hat{y}_{uv} = F(u,v | Θ, Y, G)$$\n",
    "- Y : 사용자-아이템 상호작용 행렬 (예: 클릭, 평가)\n",
    "- G : 지식 그래프 (KG), 삼중항(triple : head, relation, tail)의 집합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 KGCN Layer (모델 구성 요소)\n",
    "\n",
    "*1. 관계 중요도 계산 (사용자 u, 관계 r)*\n",
    "$$\\pi_{ur} = g(u,r)$$\n",
    "\n",
    "*2. 이웃 노드 집계 (attention 가중치 포함)*\n",
    "$$v_u^{N(v)} = \\sigma_{e\\in{N(v)}}\\tilde{\\pi}_{ur}⋅e$$\n",
    "$$\\tilde{\\pi}_{ur}=\\frac{exp(\\pi_{ur})}{\\sum\\nolimits_{e'}exp(\\pi_{ur'})}$$\n",
    "\n",
    "*3. Aggregation 방식 3가지*\n",
    "- Sum : $ReLU(W(v+v_u^{N(v)})+b)$\n",
    "- Concat : $ReLU(W[v;v_u^{N(v)}]+b)$\n",
    "- Neighbor : $ReLU(Wv_u^{N(v)}+b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Learning Algorithm (학습 알고리즘)\n",
    "\n",
    "*반복 구조*\n",
    "KGCN은 여러 계층(hop)으로 구성되어 '0-hop → 1-hop → ...'와 같은 형식으로 이웃 정보를 반복적으로 전파 및 집계\n",
    "$$\\hat{y}_{uv}=f(u, v_u^{(h)})$$\n",
    "\n",
    "*학습 손실 함수*\n",
    "- Cross Entropy + Negative Sampling + L2 정규화 포함\n",
    "$$L = \\sum_{u}\\begin{bmatrix}\\sum_{v:y_{uv}=1}J(y_{uv}, \\hat{y}_{uv})-\\sum_{i=1}^{T_u}\\mathbb{E}_{vi~P(v)}J(0,\\hat{y}_(uvi))\\end{bmatrix}+\\lambda||F||_2^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\"))\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book - Crossing 모델 설정 (Hyperparameter)\n",
    "\n",
    "|항목|설정|\n",
    "|:---:|:---:|\n",
    "임베딩 차원 | 64\n",
    "학습률 | 0.0002\n",
    "Optimizer | Adam\n",
    "정규화 계수 (L2) | 2e-5\n",
    "Negative Sampling | 1:1 비율로 샘플링\n",
    "배치 사이즈 | 256\n",
    "학습 Epoch | 최대 1000 (일반적으로 200~400)\n",
    "레이어 수 (Receptive Field Depth, H) | 1\n",
    "이웃 샘플링 수 (K) | 8\n",
    "초기화 방식 | Xavier Uniform (명시는 없지만 일반적인 초기화 방식으로 추정됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'device': 'mps',\n",
    "    'dataset': 'book',\n",
    "    'embedding_dim': 64,\n",
    "    'n_layers': 1,\n",
    "    'lr': 0.0002,\n",
    "    'batch_size': 256,\n",
    "    'l2': 2e-5,\n",
    "    'n_epoch': 200,\n",
    "    'n_neighbor': 8,\n",
    "    'H': 1,\n",
    "    'aggregator': 'sum'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    5000\n",
      "0    5000\n",
      "Name: count, dtype: int64\n",
      "Interactions: (20000263, 3)\n",
      "KG triples: 80141\n"
     ]
    }
   ],
   "source": [
    "interactions = pd.read_csv(\"data/processed/interactions.csv\")\n",
    "\n",
    "# positive / negative 각각 샘플링\n",
    "pos_sample = interactions[interactions['label'] == 1].sample(5000, random_state=42)\n",
    "neg_sample = interactions[interactions['label'] == 0].sample(5000, random_state=42)\n",
    "\n",
    "# 합치기\n",
    "small_data = pd.concat([pos_sample, neg_sample]).reset_index(drop=True)\n",
    "\n",
    "print(small_data['label'].value_counts())  # 5000 / 5000\n",
    "\n",
    "with open(\"data/processed/kg_triples.pkl\", \"rb\") as f:\n",
    "    kg_triples = pickle.load(f)\n",
    "\n",
    "with open(\"data/processed/user2id.pkl\", \"rb\") as f:\n",
    "    user2id = pickle.load(f)\n",
    "with open(\"data/processed/item2id.pkl\", \"rb\") as f:\n",
    "    item2id = pickle.load(f)\n",
    "with open(\"data/processed/entity2id.pkl\", \"rb\") as f:\n",
    "    entity2id = pickle.load(f)\n",
    "with open(\"data/processed/relation2id.pkl\", \"rb\") as f:\n",
    "    relation2id = pickle.load(f)\n",
    "\n",
    "print(f\"Interactions: {interactions.shape}\")\n",
    "print(f\"KG triples: {len(kg_triples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_kg_dict(kg_triples):\n",
    "    kg_dict = defaultdict(list)\n",
    "    for h, r, t in kg_triples:\n",
    "        kg_dict[h].append((r, t))\n",
    "    return kg_dict\n",
    "\n",
    "kg_dict = build_kg_dict(kg_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGCN(nn.Module):\n",
    "    def __init__(self, num_users, num_entities, num_relations, embed_dim, kg_dict, n_neighbors=8, n_hops=1):\n",
    "        super(KGCN, self).__init__()\n",
    "        self.user_embed = nn.Embedding(num_users, embed_dim)\n",
    "        self.entity_embed = nn.Embedding(num_entities, embed_dim)\n",
    "        self.relation_embed = nn.Embedding(num_relations, embed_dim)\n",
    "\n",
    "        self.kg = kg_dict\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.n_hops = n_hops\n",
    "\n",
    "    def sample_neighbors(self, entities):\n",
    "        batch_neighbors = []\n",
    "        batch_relations = []\n",
    "        for e in entities:\n",
    "            neighbors = self.kg.get(e.item(), [])\n",
    "            \n",
    "            # 이 부분이 없으면 문제가 발생할 수 있음\n",
    "            if len(neighbors) == 0:\n",
    "                # self-loop처럼 자기 자신을 neighbor로 추가\n",
    "                neighbors = [(0, e.item())]\n",
    "\n",
    "            # 샘플링 (길이 부족하면 마지막 이웃 반복해서 채우기)\n",
    "            sampled = neighbors[:self.n_neighbors]\n",
    "            if len(sampled) < self.n_neighbors:\n",
    "                sampled += [sampled[-1]] * (self.n_neighbors - len(sampled))\n",
    "\n",
    "            batch_neighbors.append([t for r, t in sampled])\n",
    "            batch_relations.append([r for r, t in sampled])\n",
    "\n",
    "        return torch.LongTensor(batch_neighbors), torch.LongTensor(batch_relations)\n",
    "\n",
    "\n",
    "    def aggregate(self, user_emb, entities):\n",
    "        e_embed = self.entity_embed(entities)\n",
    "        for _ in range(self.n_hops):\n",
    "            n_entities, n_relations = self.sample_neighbors(entities)\n",
    "            n_emb = self.entity_embed(n_entities.to(entities.device))\n",
    "            r_emb = self.relation_embed(n_relations.to(entities.device))\n",
    "            scores = torch.sum(user_emb.unsqueeze(1) * r_emb, dim=-1).unsqueeze(-1)\n",
    "            attn = F.softmax(scores, dim=1)\n",
    "            agg = torch.sum(attn * n_emb, dim=1)\n",
    "            e_embed = F.relu(e_embed + agg)\n",
    "        return e_embed\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        u = self.user_embed(users)\n",
    "        v = self.aggregate(u, items)\n",
    "        scores = torch.sum(u * v, dim=-1)\n",
    "        return torch.sigmoid(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KGDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = df['user'].values\n",
    "        self.items = df['item'].values\n",
    "        self.labels = df['label'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.labels[idx]\n",
    "\n",
    "train_data = KGDataset(small_data)\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 40/40 [00:01<00:00, 22.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 139.4906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 40/40 [00:01<00:00, 25.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 137.8773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = KGCN(\n",
    "    num_users=len(user2id),\n",
    "    num_entities=max(max(item2id.values()), max(entity2id.values())) + 1,\n",
    "    num_relations=len(relation2id),\n",
    "    embed_dim=64,\n",
    "    kg_dict=kg_dict\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, weight_decay=2e-5)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "for epoch in range(2):  # 2 epoch만 확인\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for users, items, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        users = users.to(device)\n",
    "        items = items.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        preds = model(users, items)\n",
    "        loss = loss_fn(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
