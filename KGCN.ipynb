{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KGCN ë…¼ë¬¸ ë¦¬ë·° & ì½”ë“œ ì‘ì„±\n",
    "**KGCN: Simplifying and Powering Graph Convolution Network for Recommendation**  \n",
    "*Hongwei Wang et al. (2019)*  \n",
    "ğŸ”— [ë…¼ë¬¸ ë§í¬](https://arxiv.org/abs/1904.12575)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Problem Formulation\n",
    "\n",
    "*ëª©í‘œ : ì‚¬ìš©ì uê°€ ì•„ì´í…œ vì— ê´€ì‹¬ ìˆì„ì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜*\n",
    "$$\\hat{y}_{uv} = F(u,v | Î˜, Y, G)$$\n",
    "- Y : ì‚¬ìš©ì-ì•„ì´í…œ ìƒí˜¸ì‘ìš© í–‰ë ¬ (ì˜ˆ: í´ë¦­, í‰ê°€)\n",
    "- G : ì§€ì‹ ê·¸ë˜í”„ (KG), ì‚¼ì¤‘í•­(triple : head, relation, tail)ì˜ ì§‘í•©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 KGCN Layer (ëª¨ë¸ êµ¬ì„± ìš”ì†Œ)\n",
    "\n",
    "*1. ê´€ê³„ ì¤‘ìš”ë„ ê³„ì‚° (ì‚¬ìš©ì u, ê´€ê³„ r)*\n",
    "$$\\pi_{ur} = g(u,r)$$\n",
    "\n",
    "*2. ì´ì›ƒ ë…¸ë“œ ì§‘ê³„ (attention ê°€ì¤‘ì¹˜ í¬í•¨)*\n",
    "$$v_u^{N(v)} = \\sigma_{e\\in{N(v)}}\\tilde{\\pi}_{ur}â‹…e$$\n",
    "$$\\tilde{\\pi}_{ur}=\\frac{exp(\\pi_{ur})}{\\sum\\nolimits_{e'}exp(\\pi_{ur'})}$$\n",
    "\n",
    "*3. Aggregation ë°©ì‹ 3ê°€ì§€*\n",
    "- Sum : $ReLU(W(v+v_u^{N(v)})+b)$\n",
    "- Concat : $ReLU(W[v;v_u^{N(v)}]+b)$\n",
    "- Neighbor : $ReLU(Wv_u^{N(v)}+b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Learning Algorithm (í•™ìŠµ ì•Œê³ ë¦¬ì¦˜)\n",
    "\n",
    "*ë°˜ë³µ êµ¬ì¡°*\n",
    "KGCNì€ ì—¬ëŸ¬ ê³„ì¸µ(hop)ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ '0-hop â†’ 1-hop â†’ ...'ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì´ì›ƒ ì •ë³´ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì „íŒŒ ë° ì§‘ê³„\n",
    "$$\\hat{y}_{uv}=f(u, v_u^{(h)})$$\n",
    "\n",
    "*í•™ìŠµ ì†ì‹¤ í•¨ìˆ˜*\n",
    "- Cross Entropy + Negative Sampling + L2 ì •ê·œí™” í¬í•¨\n",
    "$$L = \\sum_{u}\\begin{bmatrix}\\sum_{v:y_{uv}=1}J(y_{uv}, \\hat{y}_{uv})-\\sum_{i=1}^{T_u}\\mathbb{E}_{vi~P(v)}J(0,\\hat{y}_(uvi))\\end{bmatrix}+\\lambda||F||_2^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\"))\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book - Crossing ëª¨ë¸ ì„¤ì • (Hyperparameter)\n",
    "\n",
    "|í•­ëª©|ì„¤ì •|\n",
    "|:---:|:---:|\n",
    "ì„ë² ë”© ì°¨ì› | 64\n",
    "í•™ìŠµë¥  | 0.0002\n",
    "Optimizer | Adam\n",
    "ì •ê·œí™” ê³„ìˆ˜ (L2) | 2e-5\n",
    "Negative Sampling | 1:1 ë¹„ìœ¨ë¡œ ìƒ˜í”Œë§\n",
    "ë°°ì¹˜ ì‚¬ì´ì¦ˆ | 256\n",
    "í•™ìŠµ Epoch | ìµœëŒ€ 1000 (ì¼ë°˜ì ìœ¼ë¡œ 200~400)\n",
    "ë ˆì´ì–´ ìˆ˜ (Receptive Field Depth, H) | 1\n",
    "ì´ì›ƒ ìƒ˜í”Œë§ ìˆ˜ (K) | 8\n",
    "ì´ˆê¸°í™” ë°©ì‹ | Xavier Uniform (ëª…ì‹œëŠ” ì—†ì§€ë§Œ ì¼ë°˜ì ì¸ ì´ˆê¸°í™” ë°©ì‹ìœ¼ë¡œ ì¶”ì •ë¨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'device': 'mps',\n",
    "    'dataset': 'book',\n",
    "    'embedding_dim': 64,\n",
    "    'n_layers': 1,\n",
    "    'lr': 0.0002,\n",
    "    'batch_size': 256,\n",
    "    'l2': 2e-5,\n",
    "    'n_epoch': 200,\n",
    "    'n_neighbor': 8,\n",
    "    'H': 1,\n",
    "    'aggregator': 'sum'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    5000\n",
      "0    5000\n",
      "Name: count, dtype: int64\n",
      "Interactions: (20000263, 3)\n",
      "KG triples: 80141\n"
     ]
    }
   ],
   "source": [
    "interactions = pd.read_csv(\"data/processed/interactions.csv\")\n",
    "\n",
    "# positive / negative ê°ê° ìƒ˜í”Œë§\n",
    "pos_sample = interactions[interactions['label'] == 1].sample(5000, random_state=42)\n",
    "neg_sample = interactions[interactions['label'] == 0].sample(5000, random_state=42)\n",
    "\n",
    "# í•©ì¹˜ê¸°\n",
    "small_data = pd.concat([pos_sample, neg_sample]).reset_index(drop=True)\n",
    "\n",
    "print(small_data['label'].value_counts())  # 5000 / 5000\n",
    "\n",
    "with open(\"data/processed/kg_triples.pkl\", \"rb\") as f:\n",
    "    kg_triples = pickle.load(f)\n",
    "\n",
    "with open(\"data/processed/user2id.pkl\", \"rb\") as f:\n",
    "    user2id = pickle.load(f)\n",
    "with open(\"data/processed/item2id.pkl\", \"rb\") as f:\n",
    "    item2id = pickle.load(f)\n",
    "with open(\"data/processed/entity2id.pkl\", \"rb\") as f:\n",
    "    entity2id = pickle.load(f)\n",
    "with open(\"data/processed/relation2id.pkl\", \"rb\") as f:\n",
    "    relation2id = pickle.load(f)\n",
    "\n",
    "print(f\"Interactions: {interactions.shape}\")\n",
    "print(f\"KG triples: {len(kg_triples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_kg_dict(kg_triples):\n",
    "    kg_dict = defaultdict(list)\n",
    "    for h, r, t in kg_triples:\n",
    "        kg_dict[h].append((r, t))\n",
    "    return kg_dict\n",
    "\n",
    "kg_dict = build_kg_dict(kg_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGCN(nn.Module):\n",
    "    def __init__(self, num_users, num_entities, num_relations, embed_dim, kg_dict, n_neighbors=8, n_hops=1):\n",
    "        super(KGCN, self).__init__()\n",
    "        self.user_embed = nn.Embedding(num_users, embed_dim)\n",
    "        self.entity_embed = nn.Embedding(num_entities, embed_dim)\n",
    "        self.relation_embed = nn.Embedding(num_relations, embed_dim)\n",
    "\n",
    "        self.kg = kg_dict\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.n_hops = n_hops\n",
    "\n",
    "    def sample_neighbors(self, entities):\n",
    "        batch_neighbors = []\n",
    "        batch_relations = []\n",
    "        for e in entities:\n",
    "            neighbors = self.kg.get(e.item(), [])\n",
    "            \n",
    "            # ì´ ë¶€ë¶„ì´ ì—†ìœ¼ë©´ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ\n",
    "            if len(neighbors) == 0:\n",
    "                # self-loopì²˜ëŸ¼ ìê¸° ìì‹ ì„ neighborë¡œ ì¶”ê°€\n",
    "                neighbors = [(0, e.item())]\n",
    "\n",
    "            # ìƒ˜í”Œë§ (ê¸¸ì´ ë¶€ì¡±í•˜ë©´ ë§ˆì§€ë§‰ ì´ì›ƒ ë°˜ë³µí•´ì„œ ì±„ìš°ê¸°)\n",
    "            sampled = neighbors[:self.n_neighbors]\n",
    "            if len(sampled) < self.n_neighbors:\n",
    "                sampled += [sampled[-1]] * (self.n_neighbors - len(sampled))\n",
    "\n",
    "            batch_neighbors.append([t for r, t in sampled])\n",
    "            batch_relations.append([r for r, t in sampled])\n",
    "\n",
    "        return torch.LongTensor(batch_neighbors), torch.LongTensor(batch_relations)\n",
    "\n",
    "\n",
    "    def aggregate(self, user_emb, entities):\n",
    "        e_embed = self.entity_embed(entities)\n",
    "        for _ in range(self.n_hops):\n",
    "            n_entities, n_relations = self.sample_neighbors(entities)\n",
    "            n_emb = self.entity_embed(n_entities.to(entities.device))\n",
    "            r_emb = self.relation_embed(n_relations.to(entities.device))\n",
    "            scores = torch.sum(user_emb.unsqueeze(1) * r_emb, dim=-1).unsqueeze(-1)\n",
    "            attn = F.softmax(scores, dim=1)\n",
    "            agg = torch.sum(attn * n_emb, dim=1)\n",
    "            e_embed = F.relu(e_embed + agg)\n",
    "        return e_embed\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        u = self.user_embed(users)\n",
    "        v = self.aggregate(u, items)\n",
    "        scores = torch.sum(u * v, dim=-1)\n",
    "        return torch.sigmoid(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KGDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = df['user'].values\n",
    "        self.items = df['item'].values\n",
    "        self.labels = df['label'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.labels[idx]\n",
    "\n",
    "train_data = KGDataset(small_data)\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 22.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 139.4906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 25.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 137.8773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = KGCN(\n",
    "    num_users=len(user2id),\n",
    "    num_entities=max(max(item2id.values()), max(entity2id.values())) + 1,\n",
    "    num_relations=len(relation2id),\n",
    "    embed_dim=64,\n",
    "    kg_dict=kg_dict\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, weight_decay=2e-5)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "for epoch in range(2):  # 2 epochë§Œ í™•ì¸\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for users, items, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        users = users.to(device)\n",
    "        items = items.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        preds = model(users, items)\n",
    "        loss = loss_fn(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
