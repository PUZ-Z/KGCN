{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KGCN 논문 리뷰 & 코드 작성\n",
    "**KGCN: Simplifying and Powering Graph Convolution Network for Recommendation**  \n",
    "*Hongwei Wang et al. (2019)*  \n",
    "🔗 [논문 링크](https://arxiv.org/abs/1904.12575)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Problem Formulation\n",
    "\n",
    "*목표 : 사용자 u가 아이템 v에 관심 있을지를 예측하는 함수*\n",
    "$$\\hat{y}_{uv} = F(u,v | Θ, Y, G)$$\n",
    "- Y : 사용자-아이템 상호작용 행렬 (예: 클릭, 평가)\n",
    "- G : 지식 그래프 (KG), 삼중항(triple : head, relation, tail)의 집합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 KGCN Layer (모델 구성 요소)\n",
    "\n",
    "*1. 관계 중요도 계산 (사용자 u, 관계 r)*\n",
    "$$\\pi_{ur} = g(u,r)$$\n",
    "\n",
    "*2. 이웃 노드 집계 (attention 가중치 포함)*\n",
    "$$v_u^{N(v)} = \\sigma_{e\\in{N(v)}}\\tilde{\\pi}_{ur}⋅e$$\n",
    "$$\\tilde{\\pi}_{ur}=\\frac{exp(\\pi_{ur})}{\\sum\\nolimits_{e'}exp(\\pi_{ur'})}$$\n",
    "\n",
    "*3. Aggregation 방식 3가지*\n",
    "- Sum : $ReLU(W(v+v_u^{N(v)})+b)$\n",
    "- Concat : $ReLU(W[v;v_u^{N(v)}]+b)$\n",
    "- Neighbor : $ReLU(Wv_u^{N(v)}+b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Learning Algorithm (학습 알고리즘)\n",
    "\n",
    "*반복 구조*\n",
    "KGCN은 여러 계층(hop)으로 구성되어 '0-hop → 1-hop → ...'와 같은 형식으로 이웃 정보를 반복적으로 전파 및 집계\n",
    "$$\\hat{y}_{uv}=f(u, v_u^{(h)})$$\n",
    "\n",
    "*학습 손실 함수*\n",
    "- Cross Entropy + Negative Sampling + L2 정규화 포함\n",
    "$$L = \\sum_{u}\\begin{bmatrix}\\sum_{v:y_{uv}=1}J(y_{uv}, \\hat{y}_{uv})-\\sum_{i=1}^{T_u}\\mathbb{E}_{vi~P(v)}J(0,\\hat{y}_(uvi))\\end{bmatrix}+\\lambda||F||_2^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\"))\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book - Crossing 모델 설정 (Hyperparameter)\n",
    "\n",
    "|항목|설정|\n",
    "|:---:|:---:|\n",
    "임베딩 차원 | 64\n",
    "학습률 | 0.0002\n",
    "Optimizer | Adam\n",
    "정규화 계수 (L2) | 2e-5\n",
    "Negative Sampling | 1:1 비율로 샘플링\n",
    "배치 사이즈 | 256\n",
    "학습 Epoch | 최대 1000 (일반적으로 200~400)\n",
    "레이어 수 (Receptive Field Depth, H) | 1\n",
    "이웃 샘플링 수 (K) | 8\n",
    "초기화 방식 | Xavier Uniform (명시는 없지만 일반적인 초기화 방식으로 추정됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'item2id_path': 'data/item_index2entity_id.txt',\n",
    "    'kg_path': 'data/kg.txt',\n",
    "    'rating_path': 'data/user_artists.dat',\n",
    "    'rating_sep': '\\t',\n",
    "    'threshold': 0.0,\n",
    "    \n",
    "    'embedding_dim': 16,\n",
    "    'n_epochs': 20,\n",
    "    'neighbor_sample_size': 8,\n",
    "    'n_iter': 1,\n",
    "    'batch_size': 256,\n",
    "    'l2_weight': 1e-4,\n",
    "    'lr': 5e-4,\n",
    "    'train_ratio': 0.8,\n",
    "    'aggregator': 'sum',\n",
    "    'device': 'mps'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGDataLoader:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg  # 설정 저장\n",
    "\n",
    "        # 파일 로드\n",
    "        df_item2id = pd.read_csv(self.cfg['item2id_path'], sep='\\t', header=None, names=['item', 'id'])  # 아이템→엔티티 매핑\n",
    "        df_kg = pd.read_csv(self.cfg['kg_path'], sep='\\t', header=None, names=['head', 'relation', 'tail'])  # 지식 그래프\n",
    "        df_rating = pd.read_csv(self.cfg['rating_path'], sep=self.cfg['rating_sep'], \n",
    "                                names=['userID', 'itemID', 'rating'], skiprows=1)  # 사용자-아이템 평점\n",
    "\n",
    "        # 매핑에 존재하는 아이템만 필터링\n",
    "        df_rating = df_rating[df_rating['itemID'].isin(df_item2id['item'])]\n",
    "        df_rating.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        # 데이터프레임 저장\n",
    "        self.df_item2id = df_item2id\n",
    "        self.df_kg = df_kg\n",
    "        self.df_rating = df_rating\n",
    "\n",
    "        # 인코더 준비\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.entity_encoder = LabelEncoder()\n",
    "        self.relation_encoder = LabelEncoder()\n",
    "\n",
    "        # ID 인코딩 실행\n",
    "        self._encoding()\n",
    "\n",
    "    def _encoding(self): # userID, entityID, relation을 정수로 인코딩\n",
    "        self.user_encoder.fit(self.df_rating['userID'])\n",
    "        self.entity_encoder.fit(pd.concat([self.df_item2id['id'], self.df_kg['head'], self.df_kg['tail']]))\n",
    "        self.relation_encoder.fit(self.df_kg['relation'])\n",
    "\n",
    "        # 변환 적용\n",
    "        self.df_kg['head'] = self.entity_encoder.transform(self.df_kg['head'])\n",
    "        self.df_kg['tail'] = self.entity_encoder.transform(self.df_kg['tail'])\n",
    "        self.df_kg['relation'] = self.relation_encoder.transform(self.df_kg['relation'])\n",
    "\n",
    "    def _build_dataset(self): # positive + negative 샘플 생성하여 학습용 데이터셋 구성\n",
    "        print('Build dataset dataframe ...', end=' ')\n",
    "        df_dataset = pd.DataFrame()\n",
    "\n",
    "        # 사용자 ID 인코딩\n",
    "        df_dataset['userID'] = self.user_encoder.transform(self.df_rating['userID'])\n",
    "\n",
    "        # 아이템 문자열을 entity ID로 매핑\n",
    "        item2id_dict = dict(zip(self.df_item2id['item'], self.df_item2id['id']))\n",
    "        self.df_rating['itemID'] = self.df_rating['itemID'].apply(lambda x: item2id_dict[x])\n",
    "\n",
    "        # 엔티티 인코딩\n",
    "        df_dataset['itemID'] = self.entity_encoder.transform(self.df_rating['itemID'])\n",
    "\n",
    "        # 평점을 임계값 기준으로 이진 라벨 생성\n",
    "        df_dataset['label'] = self.df_rating['rating'].apply(lambda x: 0 if x < self.cfg['threshold'] else 1)\n",
    "\n",
    "        # positive만 사용\n",
    "        df_dataset = df_dataset[df_dataset['label'] == 1]\n",
    "\n",
    "        # 전체 엔티티 집합에서 negative 샘플링\n",
    "        full_item_set = set(range(len(self.entity_encoder.classes_)))\n",
    "        user_list, item_list, label_list = [], [], []\n",
    "        for user, group in df_dataset.groupby('userID'):\n",
    "            item_set = set(group['itemID'])  # positive 아이템 집합\n",
    "            negative_set = full_item_set - item_set  # negative 후보\n",
    "            negative_sampled = random.sample(list(negative_set), len(item_set))  # 같은 수 만큼 샘플링\n",
    "\n",
    "            # negative 샘플 저장\n",
    "            user_list.extend([user] * len(negative_sampled))\n",
    "            item_list.extend(negative_sampled)\n",
    "            label_list.extend([0] * len(negative_sampled))\n",
    "\n",
    "        # negative를 데이터프레임으로 만들고 결합\n",
    "        negative = pd.DataFrame({'userID': user_list, 'itemID': item_list, 'label': label_list})\n",
    "        df_dataset = pd.concat([df_dataset, negative])\n",
    "\n",
    "        # 셔플 및 인덱스 초기화\n",
    "        df_dataset = df_dataset.sample(frac=1, replace=False, random_state=999)\n",
    "        df_dataset.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        print('Done')\n",
    "        return df_dataset\n",
    "\n",
    "    def _construct_kg(self): # 양방향 knowledge graph 딕셔너리 구성\n",
    "        print('Construct knowledge graph ...', end=' ')\n",
    "        kg = dict()\n",
    "        for i in range(len(self.df_kg)):\n",
    "            head = self.df_kg.iloc[i]['head']\n",
    "            relation = self.df_kg.iloc[i]['relation']\n",
    "            tail = self.df_kg.iloc[i]['tail']\n",
    "\n",
    "            # head → tail\n",
    "            if head in kg:\n",
    "                kg[head].append((relation, tail))\n",
    "            else:\n",
    "                kg[head] = [(relation, tail)]\n",
    "\n",
    "            # tail → head (양방향)\n",
    "            if tail in kg:\n",
    "                kg[tail].append((relation, head))\n",
    "            else:\n",
    "                kg[tail] = [(relation, head)]\n",
    "\n",
    "        print('Done')\n",
    "        return kg\n",
    "\n",
    "    def load_dataset(self): # 학습용 사용자-아이템 데이터셋 로드\n",
    "        return self._build_dataset()\n",
    "\n",
    "    def load_kg(self): # 지식 그래프 딕셔너리 로드\n",
    "        return self._construct_kg()\n",
    "\n",
    "    def get_encoders(self): # user, entity, relation 인코더 반환\n",
    "        return (self.user_encoder, self.entity_encoder, self.relation_encoder)\n",
    "\n",
    "    def get_num(self): # 각 인코더의 클래스 수 반환 (모델 입력 차원 계산용)\n",
    "        return (len(self.user_encoder.classes_), \n",
    "                len(self.entity_encoder.classes_), \n",
    "                len(self.relation_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build dataset dataframe ... Done\n",
      "레이블 분포:\n",
      "label\n",
      "1    21173\n",
      "0    21173\n",
      "Name: count, dtype: int64\n",
      "\n",
      "레이블 비율 (%):\n",
      "label\n",
      "1    50.0\n",
      "0    50.0\n",
      "Name: proportion, dtype: float64\n",
      "userID    int64\n",
      "itemID    int64\n",
      "label     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 설정 정보를 담은 딕셔너리를 사용해 KGDataLoader 인스턴스를 생성\n",
    "data_loader = KGDataLoader(cfg=config)\n",
    "\n",
    "# 학습용 사용자-아이템 데이터셋을 생성 (positive + negative 샘플 포함)\n",
    "df_dataset = data_loader.load_dataset()\n",
    "\n",
    "# 레이블(positive: 1, negative: 0) 분포 출력\n",
    "print(\"레이블 분포:\")\n",
    "print(df_dataset['label'].value_counts())\n",
    "\n",
    "# 전체 데이터 중 레이블 비율 (%)로 출력\n",
    "print(\"\\n레이블 비율 (%):\")\n",
    "print(df_dataset['label'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# 데이터 타입 확인\n",
    "print(df_dataset.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, dim, aggregator):\n",
    "        super(Aggregator, self).__init__()\n",
    "        self.batch_size = batch_size  # 미니배치 크기\n",
    "        self.dim = dim                # 임베딩 차원\n",
    "        self.aggregator = aggregator  # aggregation 방식: sum / concat / neighbor-only\n",
    "\n",
    "        # aggregator 종류에 따라 파라미터 설정\n",
    "        if aggregator == 'concat':\n",
    "            # self 임베딩 + 이웃 임베딩을 concat하면 차원이 2배가 되므로\n",
    "            self.weights = torch.nn.Linear(2 * dim, dim, bias=True)\n",
    "        else:\n",
    "            # sum or neighbor-only인 경우는 차원 그대로\n",
    "            self.weights = torch.nn.Linear(dim, dim, bias=True)\n",
    "        \n",
    "    def forward(self, self_vectors, neighbor_vectors, neighbor_relations, user_embeddings, act):\n",
    "        # self_vectors: [batch_size, -1, dim]   → 현재 엔티티 (노드) 임베딩\n",
    "        # neighbor_vectors: [batch_size, -1, n_neighbor, dim] → 이웃 엔티티 임베딩\n",
    "        # neighbor_relations: [batch_size, -1, n_neighbor, dim] → 이웃 관계 임베딩\n",
    "        # user_embeddings: [batch_size, dim] → 사용자 임베딩 (user-aware attention에 사용)\n",
    "        # act: 활성화 함수 (예: torch.relu)\n",
    "        batch_size = user_embeddings.size(0)\n",
    "\n",
    "        # batch_size가 변할 수 있어 동적으로 업데이트\n",
    "        if batch_size != self.batch_size:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "        # 이웃 임베딩을 user-aware 방식으로 합치는 과정\n",
    "        neighbors_agg = self._mix_neighbor_vectors(neighbor_vectors, neighbor_relations, user_embeddings)\n",
    "\n",
    "        # aggregation 방식에 따라 self vector와 합치는 방식 달라짐\n",
    "        if self.aggregator == 'sum':\n",
    "            # self + neighbor 합산\n",
    "            output = (self_vectors + neighbors_agg).view((-1, self.dim))\n",
    "\n",
    "        elif self.aggregator == 'concat':\n",
    "            # self와 neighbor를 concat한 후 projection\n",
    "            output = torch.cat((self_vectors, neighbors_agg), dim=-1)  # [batch, -, 2*dim]\n",
    "            output = output.view((-1, 2 * self.dim))\n",
    "\n",
    "        else:\n",
    "            # neighbor-only (self vector는 사용하지 않음)\n",
    "            output = neighbors_agg.view((-1, self.dim))\n",
    "\n",
    "        # projection + 활성화\n",
    "        output = self.weights(output)  # [batch * -, dim]\n",
    "        return act(output.view((self.batch_size, -1, self.dim)))  # 다시 배치 형태로 reshape\n",
    "\n",
    "    def _mix_neighbor_vectors(self, neighbor_vectors, neighbor_relations, user_embeddings):\n",
    "        # 사용자-관계에 따라 이웃 노드들을 가중 평균함 → user-aware attention으로 neighbor vector 집계\n",
    "        # [batch, dim] → [batch, 1, 1, dim]\n",
    "        user_embeddings = user_embeddings.view((self.batch_size, 1, 1, self.dim))\n",
    "\n",
    "        # user와 relation 간의 점곱 score 계산 → [batch, -, n_neighbor]\n",
    "        user_relation_scores = (user_embeddings * neighbor_relations).sum(dim=-1)\n",
    "\n",
    "        # softmax로 attention weight 계산\n",
    "        user_relation_scores_normalized = F.softmax(user_relation_scores, dim=-1)\n",
    "\n",
    "        # [batch, -, n_neighbor] → [batch, -, n_neighbor, 1]\n",
    "        user_relation_scores_normalized = user_relation_scores_normalized.unsqueeze(dim=-1)\n",
    "\n",
    "        # attention weight * neighbor vector → 가중 평균 [batch, -, dim]\n",
    "        neighbors_aggregated = (user_relation_scores_normalized * neighbor_vectors).sum(dim=2)\n",
    "\n",
    "        return neighbors_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGCN(torch.nn.Module):\n",
    "    def __init__(self, num_user, num_ent, num_rel, kg, config):\n",
    "        super(KGCN, self).__init__()\n",
    "        self.num_user = num_user\n",
    "        self.num_ent = num_ent\n",
    "        self.num_rel = num_rel\n",
    "        self.n_iter = config['n_iter']                           # GCN 레이어 수\n",
    "        self.batch_size = config['batch_size']                  # 배치 크기\n",
    "        self.dim = config['embedding_dim']                      # 임베딩 차원\n",
    "        self.n_neighbor = config['neighbor_sample_size']        # 이웃 수\n",
    "        self.kg = kg\n",
    "        self.device = torch.device(config['device'])            # device 설정\n",
    "\n",
    "        # Aggregator 인스턴스\n",
    "        self.aggregator = Aggregator(self.batch_size, self.dim, config['aggregator'])\n",
    "\n",
    "        self._gen_adj()\n",
    "\n",
    "        # 임베딩 레이어\n",
    "        self.usr = torch.nn.Embedding(num_user, self.dim)\n",
    "        self.ent = torch.nn.Embedding(num_ent, self.dim)\n",
    "        self.rel = torch.nn.Embedding(num_rel, self.dim)\n",
    "\n",
    "    def _gen_adj(self): # 엔티티마다 고정 개수의 이웃 엔티티 및 관계를 샘플링하여 인접행렬 생성\n",
    "        self.adj_ent = torch.empty(self.num_ent, self.n_neighbor, dtype=torch.long)\n",
    "        self.adj_rel = torch.empty(self.num_ent, self.n_neighbor, dtype=torch.long)\n",
    "\n",
    "        for e in self.kg:\n",
    "            if len(self.kg[e]) >= self.n_neighbor:\n",
    "                neighbors = random.sample(self.kg[e], self.n_neighbor)\n",
    "            else:\n",
    "                neighbors = random.choices(self.kg[e], k=self.n_neighbor)  # 중복 허용 샘플링\n",
    "\n",
    "            self.adj_ent[e] = torch.LongTensor([ent for _, ent in neighbors])\n",
    "            self.adj_rel[e] = torch.LongTensor([rel for rel, _ in neighbors])\n",
    "\n",
    "    def forward(self, u, v): # forward 호출 시 (user, item) 쌍 입력 → user-aware item score 출력 / u: [batch_size], v: [batch_size]\n",
    "        batch_size = u.size(0)\n",
    "        if batch_size != self.batch_size:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "        # shape 맞추기: [batch_size, 1]\n",
    "        u = u.view((-1, 1))\n",
    "        v = v.view((-1, 1))\n",
    "\n",
    "        # 사용자 임베딩: [batch_size, dim]\n",
    "        user_embeddings = self.usr(u).squeeze(dim=1)\n",
    "\n",
    "        # item을 기준으로 multi-hop 이웃 엔티티/관계 가져오기\n",
    "        entities, relations = self._get_neighbors(v)\n",
    "\n",
    "        # 이웃 정보를 user-aware 방식으로 aggregation\n",
    "        item_embeddings = self._aggregate(user_embeddings, entities, relations)\n",
    "\n",
    "        # 최종 user-item score 계산: 내적 후 sigmoid\n",
    "        scores = (user_embeddings * item_embeddings).sum(dim=1)\n",
    "\n",
    "        return torch.sigmoid(scores)\n",
    "\n",
    "    def _get_neighbors(self, v): # 엔티티 v의 multi-hop 이웃들을 adj matrix 기반으로 샘플링 / v: [batch_size, 1] → 1-hop, 2-hop ...까지 쌓임\n",
    "        entities = [v]  # 0-hop (자기 자신)\n",
    "        relations = []\n",
    "\n",
    "        for h in range(self.n_iter):  # hop 수만큼 반복\n",
    "            # 현재 hop의 엔티티에서 이웃 추출\n",
    "            neighbor_entities = torch.LongTensor(self.adj_ent[entities[h].cpu()]) \\\n",
    "                                    .view((self.batch_size, -1)).to(self.device)\n",
    "            neighbor_relations = torch.LongTensor(self.adj_rel[entities[h].cpu()]) \\\n",
    "                                    .view((self.batch_size, -1)).to(self.device)\n",
    "\n",
    "            entities.append(neighbor_entities)\n",
    "            relations.append(neighbor_relations)\n",
    "\n",
    "        return entities, relations\n",
    "\n",
    "    def _aggregate(self, user_embeddings, entities, relations): # Aggregator를 사용해 multi-hop 이웃 정보를 통합\n",
    "        # user_embeddings: [batch_size, dim]\n",
    "        # entities: hop별 entity 리스트\n",
    "        # relations: hop별 relation 리스트\n",
    "        \n",
    "        # hop 별로 entity / relation 임베딩 추출\n",
    "        entity_vectors = [self.ent(entity) for entity in entities]\n",
    "        relation_vectors = [self.rel(relation) for relation in relations]\n",
    "\n",
    "        # hop 수만큼 반복적으로 aggregation\n",
    "        for i in range(self.n_iter):\n",
    "            # 마지막 hop은 tanh, 그 외는 sigmoid\n",
    "            act = torch.tanh if i == self.n_iter - 1 else torch.sigmoid\n",
    "\n",
    "            entity_vectors_next_iter = []\n",
    "            for hop in range(self.n_iter - i):\n",
    "                vector = self.aggregator(\n",
    "                    self_vectors=entity_vectors[hop],\n",
    "                    neighbor_vectors=entity_vectors[hop + 1] \\\n",
    "                        .view((self.batch_size, -1, self.n_neighbor, self.dim)),\n",
    "                    neighbor_relations=relation_vectors[hop] \\\n",
    "                        .view((self.batch_size, -1, self.n_neighbor, self.dim)),\n",
    "                    user_embeddings=user_embeddings,\n",
    "                    act=act\n",
    "                )\n",
    "                entity_vectors_next_iter.append(vector)\n",
    "\n",
    "            # 다음 hop의 결과로 갱신\n",
    "            entity_vectors = entity_vectors_next_iter\n",
    "\n",
    "        # 최종 item 임베딩 반환\n",
    "        return entity_vectors[0].view((self.batch_size, self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct knowledge graph ... Done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1217</td>\n",
       "      <td>289</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1648</td>\n",
       "      <td>4018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>596</td>\n",
       "      <td>8885</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>475</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1450</td>\n",
       "      <td>5171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42341</th>\n",
       "      <td>1778</td>\n",
       "      <td>1663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42342</th>\n",
       "      <td>519</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42343</th>\n",
       "      <td>1487</td>\n",
       "      <td>2879</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42344</th>\n",
       "      <td>1115</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42345</th>\n",
       "      <td>793</td>\n",
       "      <td>709</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42346 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userID  itemID  label\n",
       "0        1217     289      1\n",
       "1        1648    4018      0\n",
       "2         596    8885      0\n",
       "3         475      57      1\n",
       "4        1450    5171      0\n",
       "...       ...     ...    ...\n",
       "42341    1778    1663      0\n",
       "42342     519       4      1\n",
       "42343    1487    2879      1\n",
       "42344    1115      36      1\n",
       "42345     793     709      0\n",
       "\n",
       "[42346 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg = data_loader.load_kg()\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset 클래스 정의\n",
    "class KGCNDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df): # df: 학습용 데이터프레임 (userID, itemID, label 포함)\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self): # 전체 샘플 개수 반환 → DataLoader에서 전체 데이터 크기 인식용\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx): # 인덱스 idx에 해당하는 샘플을 반환 → DataLoader가 배치 단위로 호출할 때 사용됨\n",
    "        user_id = np.array(self.df.iloc[idx]['userID']) # 사용자 ID\n",
    "        item_id = np.array(self.df.iloc[idx]['itemID']) # 아이템 (entity) ID\n",
    "        label = np.array(self.df.iloc[idx]['label'], dtype=np.float32) # 이진 라벨 (float32로 변환)\n",
    "\n",
    "        return user_id, item_id, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct knowledge graph ... Done\n",
      "모델 및 데이터셋 준비 완료\n"
     ]
    }
   ],
   "source": [
    "# 텐서 데이터셋 구성\n",
    "train_data = TensorDataset(\n",
    "    torch.LongTensor(df_dataset['userID'].values),\n",
    "    torch.LongTensor(df_dataset['itemID'].values),\n",
    "    torch.FloatTensor(df_dataset['label'].values)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "# 모델 생성에 필요한 수치 정보\n",
    "n_user, n_entity, n_relation = data_loader.get_num()\n",
    "kg_dict = data_loader.load_kg()\n",
    "\n",
    "# 모델 생성 (config 통합 버전의 KGCN 클래스 사용)\n",
    "model = KGCN(num_user=n_user, num_ent=n_entity, num_rel=n_relation, kg=kg_dict, config=config).to(device)\n",
    "\n",
    "print(\"모델 및 데이터셋 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할 (config['train_ratio'] 사용)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df_dataset,\n",
    "    df_dataset['label'],\n",
    "    test_size=1 - config['train_ratio'],\n",
    "    shuffle=False,               # 시계열 데이터면 False, 랜덤이면 True\n",
    "    random_state=999             # 재현성 보장을 위한 고정 seed\n",
    ")\n",
    "\n",
    "# 커스텀 Dataset 클래스 활용\n",
    "train_dataset = KGCNDataset(x_train)\n",
    "test_dataset = KGCNDataset(x_test)\n",
    "\n",
    "# DataLoader 구성 (config 사용)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "# 사용자, 엔티티, 관계 수 얻기\n",
    "num_user, num_entity, num_relation = data_loader.get_num()\n",
    "\n",
    "# 인코더들 (원한다면 추후 디코딩에 사용 가능)\n",
    "user_encoder, entity_encoder, relation_encoder = data_loader.get_encoders()\n",
    "\n",
    "# KGCN 모델 생성 (config 기반)\n",
    "net = KGCN(num_user, num_entity, num_relation, kg_dict, config).to(device)\n",
    "\n",
    "# Binary Cross Entropy Loss (sigmoid + float label에 적합)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# Adam 옵티마이저 (학습률 + L2 정규화는 config에서)\n",
    "optimizer = optim.Adam(\n",
    "    net.parameters(),\n",
    "    lr=config['lr'],\n",
    "    weight_decay=config['l2_weight']\n",
    ")\n",
    "\n",
    "print('device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 1.0593\n",
      "[Epoch 1] Test Loss: 1.0450 | AUC: 0.4985\n",
      "[Epoch 2] Train Loss: 0.9810\n",
      "[Epoch 2] Test Loss: 0.9757 | AUC: 0.4980\n",
      "[Epoch 3] Train Loss: 0.9045\n",
      "[Epoch 3] Test Loss: 0.9077 | AUC: 0.4978\n",
      "[Epoch 4] Train Loss: 0.8360\n",
      "[Epoch 4] Test Loss: 0.8460 | AUC: 0.4967\n",
      "[Epoch 5] Train Loss: 0.7820\n",
      "[Epoch 5] Test Loss: 0.7957 | AUC: 0.4950\n",
      "[Epoch 6] Train Loss: 0.7414\n",
      "[Epoch 6] Test Loss: 0.7594 | AUC: 0.4933\n",
      "[Epoch 7] Train Loss: 0.7163\n",
      "[Epoch 7] Test Loss: 0.7357 | AUC: 0.4906\n",
      "[Epoch 8] Train Loss: 0.7017\n",
      "[Epoch 8] Test Loss: 0.7219 | AUC: 0.4883\n",
      "[Epoch 9] Train Loss: 0.6943\n",
      "[Epoch 9] Test Loss: 0.7140 | AUC: 0.4879\n",
      "[Epoch 10] Train Loss: 0.6903\n",
      "[Epoch 10] Test Loss: 0.7094 | AUC: 0.4864\n",
      "[Epoch 11] Train Loss: 0.6884\n",
      "[Epoch 11] Test Loss: 0.7067 | AUC: 0.4868\n",
      "[Epoch 12] Train Loss: 0.6874\n",
      "[Epoch 12] Test Loss: 0.7048 | AUC: 0.4870\n",
      "[Epoch 13] Train Loss: 0.6866\n",
      "[Epoch 13] Test Loss: 0.7039 | AUC: 0.4859\n",
      "[Epoch 14] Train Loss: 0.6859\n",
      "[Epoch 14] Test Loss: 0.7030 | AUC: 0.4864\n",
      "[Epoch 15] Train Loss: 0.6855\n",
      "[Epoch 15] Test Loss: 0.7023 | AUC: 0.4860\n",
      "[Epoch 16] Train Loss: 0.6851\n",
      "[Epoch 16] Test Loss: 0.7018 | AUC: 0.4871\n",
      "[Epoch 17] Train Loss: 0.6844\n",
      "[Epoch 17] Test Loss: 0.7015 | AUC: 0.4874\n",
      "[Epoch 18] Train Loss: 0.6839\n",
      "[Epoch 18] Test Loss: 0.7012 | AUC: 0.4876\n",
      "[Epoch 19] Train Loss: 0.6833\n",
      "[Epoch 19] Test Loss: 0.7009 | AUC: 0.4876\n",
      "[Epoch 20] Train Loss: 0.6825\n",
      "[Epoch 20] Test Loss: 0.7005 | AUC: 0.4889\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "test_loss_list = []\n",
    "auc_score_list = []\n",
    "\n",
    "# 학습 에폭 반복\n",
    "for epoch in range(config['n_epochs']):\n",
    "    net.train()  # 학습 모드\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # 1 epoch 동안 전체 배치 학습\n",
    "    for user_ids, item_ids, labels in train_loader:\n",
    "        user_ids, item_ids, labels = user_ids.to(device), item_ids.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(user_ids, item_ids)  # forward\n",
    "        loss = criterion(outputs, labels)  # BCE loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # 에폭별 평균 학습 손실 저장 및 출력\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    loss_list.append(avg_train_loss)\n",
    "    print(f'[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        total_auc = 0\n",
    "\n",
    "        for user_ids, item_ids, labels in test_loader:\n",
    "            user_ids, item_ids, labels = user_ids.to(device), item_ids.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(user_ids, item_ids)\n",
    "            test_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            # AUC 계산 (label, 예측 확률)\n",
    "            total_auc += roc_auc_score(labels.cpu().numpy(), outputs.cpu().numpy())\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        avg_auc = total_auc / len(test_loader)\n",
    "\n",
    "        test_loss_list.append(avg_test_loss)\n",
    "        auc_score_list.append(avg_auc)\n",
    "\n",
    "        print(f'[Epoch {epoch+1}] Test Loss: {avg_test_loss:.4f} | AUC: {avg_auc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
