{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KGCN 논문 리뷰 & 코드 작성\n",
    "**KGCN: Simplifying and Powering Graph Convolution Network for Recommendation**  \n",
    "*Hongwei Wang et al. (2019)*  \n",
    "🔗 [논문 링크](https://arxiv.org/abs/1904.12575)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Problem Formulation\n",
    "\n",
    "*목표 : 사용자 u가 아이템 v에 관심 있을지를 예측하는 함수*\n",
    "$$\\hat{y}_{uv} = F(u,v | Θ, Y, G)$$\n",
    "- Y : 사용자-아이템 상호작용 행렬 (예: 클릭, 평가)\n",
    "- G : 지식 그래프 (KG), 삼중항(triple : head, relation, tail)의 집합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 KGCN Layer (모델 구성 요소)\n",
    "\n",
    "*1. 관계 중요도 계산 (사용자 u, 관계 r)*\n",
    "$$\\pi_{ur} = g(u,r)$$\n",
    "\n",
    "*2. 이웃 노드 집계 (attention 가중치 포함)*\n",
    "$$v_u^{N(v)} = \\sigma_{e\\in{N(v)}}\\tilde{\\pi}_{ur}⋅e$$\n",
    "$$\\tilde{\\pi}_{ur}=\\frac{exp(\\pi_{ur})}{\\sum\\nolimits_{e'}exp(\\pi_{ur'})}$$\n",
    "\n",
    "*3. Aggregation 방식 3가지*\n",
    "- Sum : $ReLU(W(v+v_u^{N(v)})+b)$\n",
    "- Concat : $ReLU(W[v;v_u^{N(v)}]+b)$\n",
    "- Neighbor : $ReLU(Wv_u^{N(v)}+b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Learning Algorithm (학습 알고리즘)\n",
    "\n",
    "*반복 구조*\n",
    "KGCN은 여러 계층(hop)으로 구성되어 '0-hop → 1-hop → ...'와 같은 형식으로 이웃 정보를 반복적으로 전파 및 집계\n",
    "$$\\hat{y}_{uv}=f(u, v_u^{(h)})$$\n",
    "\n",
    "*학습 손실 함수*\n",
    "- Cross Entropy + Negative Sampling + L2 정규화 포함\n",
    "$$L = \\sum_{u}\\begin{bmatrix}\\sum_{v:y_{uv}=1}J(y_{uv}, \\hat{y}_{uv})-\\sum_{i=1}^{T_u}\\mathbb{E}_{vi~P(v)}J(0,\\hat{y}_(uvi))\\end{bmatrix}+\\lambda||F||_2^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\"))\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book - Crossing 모델 설정 (Hyperparameter)\n",
    "\n",
    "|항목|설정|\n",
    "|:---:|:---:|\n",
    "임베딩 차원 | 64\n",
    "학습률 | 0.0002\n",
    "Optimizer | Adam\n",
    "정규화 계수 (L2) | 2e-5\n",
    "Negative Sampling | 1:1 비율로 샘플링\n",
    "배치 사이즈 | 256\n",
    "학습 Epoch | 최대 1000 (일반적으로 200~400)\n",
    "레이어 수 (Receptive Field Depth, H) | 1\n",
    "이웃 샘플링 수 (K) | 8\n",
    "초기화 방식 | Xavier Uniform (명시는 없지만 일반적인 초기화 방식으로 추정됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "config = {\n",
    "    'device': 'mps',\n",
    "    'dataset': 'book',\n",
    "    'embedding_dim': 64,\n",
    "    'n_layers': 1,\n",
    "    'lr': 0.0002,\n",
    "    'batch_size': 256,\n",
    "    'l2': 2e-5,\n",
    "    'n_epoch': 200,\n",
    "    'n_neighbor': 8,\n",
    "    'H': 1,\n",
    "    'aggregator': 'sum'\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'item2id_path': 'data/item_index2entity_id.txt',\n",
    "    'kg_path': 'data/kg.txt',\n",
    "    'rating_path': 'data/user_artists.dat',\n",
    "    'rating_sep': '\\t',\n",
    "    'threshold': 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGDataLoader:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg  # 설정 저장\n",
    "\n",
    "        # 파일 로드\n",
    "        df_item2id = pd.read_csv(self.cfg['item2id_path'], sep='\\t', header=None, names=['item', 'id'])  # 아이템→엔티티 매핑\n",
    "        df_kg = pd.read_csv(self.cfg['kg_path'], sep='\\t', header=None, names=['head', 'relation', 'tail'])  # 지식 그래프\n",
    "        df_rating = pd.read_csv(self.cfg['rating_path'], sep=self.cfg['rating_sep'], \n",
    "                                names=['userID', 'itemID', 'rating'], skiprows=1)  # 사용자-아이템 평점\n",
    "\n",
    "        # 매핑에 존재하는 아이템만 필터링\n",
    "        df_rating = df_rating[df_rating['itemID'].isin(df_item2id['item'])]\n",
    "        df_rating.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        # 데이터프레임 저장\n",
    "        self.df_item2id = df_item2id\n",
    "        self.df_kg = df_kg\n",
    "        self.df_rating = df_rating\n",
    "\n",
    "        # 인코더 준비\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.entity_encoder = LabelEncoder()\n",
    "        self.relation_encoder = LabelEncoder()\n",
    "\n",
    "        # ID 인코딩 실행\n",
    "        self._encoding()\n",
    "\n",
    "    def _encoding(self): # userID, entityID, relation을 정수로 인코딩\n",
    "        self.user_encoder.fit(self.df_rating['userID'])\n",
    "        self.entity_encoder.fit(pd.concat([self.df_item2id['id'], self.df_kg['head'], self.df_kg['tail']]))\n",
    "        self.relation_encoder.fit(self.df_kg['relation'])\n",
    "\n",
    "        # 변환 적용\n",
    "        self.df_kg['head'] = self.entity_encoder.transform(self.df_kg['head'])\n",
    "        self.df_kg['tail'] = self.entity_encoder.transform(self.df_kg['tail'])\n",
    "        self.df_kg['relation'] = self.relation_encoder.transform(self.df_kg['relation'])\n",
    "\n",
    "    def _build_dataset(self): # positive + negative 샘플 생성하여 학습용 데이터셋 구성\n",
    "        print('Build dataset dataframe ...', end=' ')\n",
    "        df_dataset = pd.DataFrame()\n",
    "\n",
    "        # 사용자 ID 인코딩\n",
    "        df_dataset['userID'] = self.user_encoder.transform(self.df_rating['userID'])\n",
    "\n",
    "        # 아이템 문자열을 entity ID로 매핑\n",
    "        item2id_dict = dict(zip(self.df_item2id['item'], self.df_item2id['id']))\n",
    "        self.df_rating['itemID'] = self.df_rating['itemID'].apply(lambda x: item2id_dict[x])\n",
    "\n",
    "        # 엔티티 인코딩\n",
    "        df_dataset['itemID'] = self.entity_encoder.transform(self.df_rating['itemID'])\n",
    "\n",
    "        # 평점을 임계값 기준으로 이진 라벨 생성\n",
    "        df_dataset['label'] = self.df_rating['rating'].apply(lambda x: 0 if x < self.cfg['threshold'] else 1)\n",
    "\n",
    "        # positive만 사용\n",
    "        df_dataset = df_dataset[df_dataset['label'] == 1]\n",
    "\n",
    "        # 전체 엔티티 집합에서 negative 샘플링\n",
    "        full_item_set = set(range(len(self.entity_encoder.classes_)))\n",
    "        user_list, item_list, label_list = [], [], []\n",
    "        for user, group in df_dataset.groupby('userID'):\n",
    "            item_set = set(group['itemID'])  # positive 아이템 집합\n",
    "            negative_set = full_item_set - item_set  # negative 후보\n",
    "            negative_sampled = random.sample(list(negative_set), len(item_set))  # 같은 수 만큼 샘플링\n",
    "\n",
    "            # negative 샘플 저장\n",
    "            user_list.extend([user] * len(negative_sampled))\n",
    "            item_list.extend(negative_sampled)\n",
    "            label_list.extend([0] * len(negative_sampled))\n",
    "\n",
    "        # negative를 데이터프레임으로 만들고 결합\n",
    "        negative = pd.DataFrame({'userID': user_list, 'itemID': item_list, 'label': label_list})\n",
    "        df_dataset = pd.concat([df_dataset, negative])\n",
    "\n",
    "        # 셔플 및 인덱스 초기화\n",
    "        df_dataset = df_dataset.sample(frac=1, replace=False, random_state=999)\n",
    "        df_dataset.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        print('Done')\n",
    "        return df_dataset\n",
    "\n",
    "    def _construct_kg(self): # 양방향 knowledge graph 딕셔너리 구성\n",
    "        print('Construct knowledge graph ...', end=' ')\n",
    "        kg = dict()\n",
    "        for i in range(len(self.df_kg)):\n",
    "            head = self.df_kg.iloc[i]['head']\n",
    "            relation = self.df_kg.iloc[i]['relation']\n",
    "            tail = self.df_kg.iloc[i]['tail']\n",
    "\n",
    "            # head → tail\n",
    "            if head in kg:\n",
    "                kg[head].append((relation, tail))\n",
    "            else:\n",
    "                kg[head] = [(relation, tail)]\n",
    "\n",
    "            # tail → head (양방향)\n",
    "            if tail in kg:\n",
    "                kg[tail].append((relation, head))\n",
    "            else:\n",
    "                kg[tail] = [(relation, head)]\n",
    "\n",
    "        print('Done')\n",
    "        return kg\n",
    "\n",
    "    def load_dataset(self): # 학습용 사용자-아이템 데이터셋 로드\n",
    "        return self._build_dataset()\n",
    "\n",
    "    def load_kg(self): # 지식 그래프 딕셔너리 로드\n",
    "        return self._construct_kg()\n",
    "\n",
    "    def get_encoders(self): # user, entity, relation 인코더 반환\n",
    "        return (self.user_encoder, self.entity_encoder, self.relation_encoder)\n",
    "\n",
    "    def get_num(self): # 각 인코더의 클래스 수 반환 (모델 입력 차원 계산용)\n",
    "        return (len(self.user_encoder.classes_), \n",
    "                len(self.entity_encoder.classes_), \n",
    "                len(self.relation_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정 정보를 담은 딕셔너리를 사용해 KGDataLoader 인스턴스를 생성\n",
    "data_loader = KGDataLoader(cfg=config)\n",
    "\n",
    "# 학습용 사용자-아이템 데이터셋을 생성 (positive + negative 샘플 포함)\n",
    "df_dataset = data_loader.load_dataset()\n",
    "\n",
    "# 레이블(positive: 1, negative: 0) 분포 출력\n",
    "print(\"레이블 분포:\")\n",
    "print(df_dataset['label'].value_counts())\n",
    "\n",
    "# 전체 데이터 중 레이블 비율 (%)로 출력\n",
    "print(\"\\n레이블 비율 (%):\")\n",
    "print(df_dataset['label'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = data_loader.load_dataset()\n",
    "# 데이터 타입 확인\n",
    "print(df_dataset.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, dim, aggregator):\n",
    "        super(Aggregator, self).__init__()\n",
    "        self.batch_size = batch_size  # 미니배치 크기\n",
    "        self.dim = dim                # 임베딩 차원\n",
    "        self.aggregator = aggregator  # aggregation 방식: sum / concat / neighbor-only\n",
    "\n",
    "        # aggregator 종류에 따라 파라미터 설정\n",
    "        if aggregator == 'concat':\n",
    "            # self 임베딩 + 이웃 임베딩을 concat하면 차원이 2배가 되므로\n",
    "            self.weights = torch.nn.Linear(2 * dim, dim, bias=True)\n",
    "        else:\n",
    "            # sum or neighbor-only인 경우는 차원 그대로\n",
    "            self.weights = torch.nn.Linear(dim, dim, bias=True)\n",
    "        \n",
    "    def forward(self, self_vectors, neighbor_vectors, neighbor_relations, user_embeddings, act):\n",
    "        # self_vectors: [batch_size, -1, dim]   → 현재 엔티티 (노드) 임베딩\n",
    "        # neighbor_vectors: [batch_size, -1, n_neighbor, dim] → 이웃 엔티티 임베딩\n",
    "        # neighbor_relations: [batch_size, -1, n_neighbor, dim] → 이웃 관계 임베딩\n",
    "        # user_embeddings: [batch_size, dim] → 사용자 임베딩 (user-aware attention에 사용)\n",
    "        # act: 활성화 함수 (예: torch.relu)\n",
    "        batch_size = user_embeddings.size(0)\n",
    "\n",
    "        # batch_size가 변할 수 있어 동적으로 업데이트\n",
    "        if batch_size != self.batch_size:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "        # 이웃 임베딩을 user-aware 방식으로 합치는 과정\n",
    "        neighbors_agg = self._mix_neighbor_vectors(neighbor_vectors, neighbor_relations, user_embeddings)\n",
    "\n",
    "        # aggregation 방식에 따라 self vector와 합치는 방식 달라짐\n",
    "        if self.aggregator == 'sum':\n",
    "            # self + neighbor 합산\n",
    "            output = (self_vectors + neighbors_agg).view((-1, self.dim))\n",
    "\n",
    "        elif self.aggregator == 'concat':\n",
    "            # self와 neighbor를 concat한 후 projection\n",
    "            output = torch.cat((self_vectors, neighbors_agg), dim=-1)  # [batch, -, 2*dim]\n",
    "            output = output.view((-1, 2 * self.dim))\n",
    "\n",
    "        else:\n",
    "            # neighbor-only (self vector는 사용하지 않음)\n",
    "            output = neighbors_agg.view((-1, self.dim))\n",
    "\n",
    "        # projection + 활성화\n",
    "        output = self.weights(output)  # [batch * -, dim]\n",
    "        return act(output.view((self.batch_size, -1, self.dim)))  # 다시 배치 형태로 reshape\n",
    "\n",
    "    def _mix_neighbor_vectors(self, neighbor_vectors, neighbor_relations, user_embeddings):\n",
    "        # 사용자-관계에 따라 이웃 노드들을 가중 평균함 → user-aware attention으로 neighbor vector 집계\n",
    "        # [batch, dim] → [batch, 1, 1, dim]\n",
    "        user_embeddings = user_embeddings.view((self.batch_size, 1, 1, self.dim))\n",
    "\n",
    "        # user와 relation 간의 점곱 score 계산 → [batch, -, n_neighbor]\n",
    "        user_relation_scores = (user_embeddings * neighbor_relations).sum(dim=-1)\n",
    "\n",
    "        # softmax로 attention weight 계산\n",
    "        user_relation_scores_normalized = F.softmax(user_relation_scores, dim=-1)\n",
    "\n",
    "        # [batch, -, n_neighbor] → [batch, -, n_neighbor, 1]\n",
    "        user_relation_scores_normalized = user_relation_scores_normalized.unsqueeze(dim=-1)\n",
    "\n",
    "        # attention weight * neighbor vector → 가중 평균 [batch, -, dim]\n",
    "        neighbors_aggregated = (user_relation_scores_normalized * neighbor_vectors).sum(dim=2)\n",
    "\n",
    "        return neighbors_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGCN(torch.nn.Module):\n",
    "    def __init__(self, num_user, num_ent, num_rel, kg, args, device):\n",
    "        super(KGCN, self).__init__()\n",
    "        self.num_user = num_user          # 사용자 수\n",
    "        self.num_ent = num_ent            # 엔티티 수\n",
    "        self.num_rel = num_rel            # 관계 수\n",
    "        self.n_iter = args.n_iter         # GCN 레이어 수 (= neighbor 탐색 깊이)\n",
    "        self.batch_size = args.batch_size\n",
    "        self.dim = args.dim               # 임베딩 차원\n",
    "        self.n_neighbor = args.neighbor_sample_size  # 각 노드마다 샘플링할 이웃 수\n",
    "        self.kg = kg                      # 지식 그래프 (dict)\n",
    "        self.device = device              # GPU/CPU\n",
    "        self.aggregator = Aggregator(self.batch_size, self.dim, args.aggregator)  # Aggregator 모듈\n",
    "\n",
    "        self._gen_adj()  # 인접 엔티티/관계 샘플링 테이블 생성\n",
    "\n",
    "        # Embedding layer: 사용자, 엔티티, 관계\n",
    "        self.usr = torch.nn.Embedding(num_user, args.dim)\n",
    "        self.ent = torch.nn.Embedding(num_ent, args.dim)\n",
    "        self.rel = torch.nn.Embedding(num_rel, args.dim)\n",
    "\n",
    "    def _gen_adj(self): # 엔티티마다 고정 개수의 이웃 엔티티 및 관계를 샘플링하여 인접행렬 생성\n",
    "        self.adj_ent = torch.empty(self.num_ent, self.n_neighbor, dtype=torch.long)\n",
    "        self.adj_rel = torch.empty(self.num_ent, self.n_neighbor, dtype=torch.long)\n",
    "\n",
    "        for e in self.kg:\n",
    "            if len(self.kg[e]) >= self.n_neighbor:\n",
    "                neighbors = random.sample(self.kg[e], self.n_neighbor)\n",
    "            else:\n",
    "                neighbors = random.choices(self.kg[e], k=self.n_neighbor)  # 중복 허용 샘플링\n",
    "\n",
    "            self.adj_ent[e] = torch.LongTensor([ent for _, ent in neighbors])\n",
    "            self.adj_rel[e] = torch.LongTensor([rel for rel, _ in neighbors])\n",
    "\n",
    "    def forward(self, u, v): # forward 호출 시 (user, item) 쌍 입력 → user-aware item score 출력 / u: [batch_size], v: [batch_size]\n",
    "        batch_size = u.size(0)\n",
    "        if batch_size != self.batch_size:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "        # shape 맞추기: [batch_size, 1]\n",
    "        u = u.view((-1, 1))\n",
    "        v = v.view((-1, 1))\n",
    "\n",
    "        # 사용자 임베딩: [batch_size, dim]\n",
    "        user_embeddings = self.usr(u).squeeze(dim=1)\n",
    "\n",
    "        # item을 기준으로 multi-hop 이웃 엔티티/관계 가져오기\n",
    "        entities, relations = self._get_neighbors(v)\n",
    "\n",
    "        # 이웃 정보를 user-aware 방식으로 aggregation\n",
    "        item_embeddings = self._aggregate(user_embeddings, entities, relations)\n",
    "\n",
    "        # 최종 user-item score 계산: 내적 후 sigmoid\n",
    "        scores = (user_embeddings * item_embeddings).sum(dim=1)\n",
    "\n",
    "        return torch.sigmoid(scores)\n",
    "\n",
    "    def _get_neighbors(self, v): # 엔티티 v의 multi-hop 이웃들을 adj matrix 기반으로 샘플링 / v: [batch_size, 1] → 1-hop, 2-hop ...까지 쌓임\n",
    "        entities = [v]  # 0-hop (자기 자신)\n",
    "        relations = []\n",
    "\n",
    "        for h in range(self.n_iter):  # hop 수만큼 반복\n",
    "            # 현재 hop의 엔티티에서 이웃 추출\n",
    "            neighbor_entities = torch.LongTensor(self.adj_ent[entities[h].cpu()]) \\\n",
    "                                    .view((self.batch_size, -1)).to(self.device)\n",
    "            neighbor_relations = torch.LongTensor(self.adj_rel[entities[h].cpu()]) \\\n",
    "                                    .view((self.batch_size, -1)).to(self.device)\n",
    "\n",
    "            entities.append(neighbor_entities)\n",
    "            relations.append(neighbor_relations)\n",
    "\n",
    "        return entities, relations\n",
    "\n",
    "    def _aggregate(self, user_embeddings, entities, relations): # Aggregator를 사용해 multi-hop 이웃 정보를 통합\n",
    "        # user_embeddings: [batch_size, dim]\n",
    "        # entities: hop별 entity 리스트\n",
    "        # relations: hop별 relation 리스트\n",
    "        \n",
    "        # hop 별로 entity / relation 임베딩 추출\n",
    "        entity_vectors = [self.ent(entity) for entity in entities]\n",
    "        relation_vectors = [self.rel(relation) for relation in relations]\n",
    "\n",
    "        # hop 수만큼 반복적으로 aggregation\n",
    "        for i in range(self.n_iter):\n",
    "            # 마지막 hop은 tanh, 그 외는 sigmoid\n",
    "            act = torch.tanh if i == self.n_iter - 1 else torch.sigmoid\n",
    "\n",
    "            entity_vectors_next_iter = []\n",
    "            for hop in range(self.n_iter - i):\n",
    "                vector = self.aggregator(\n",
    "                    self_vectors=entity_vectors[hop],\n",
    "                    neighbor_vectors=entity_vectors[hop + 1] \\\n",
    "                        .view((self.batch_size, -1, self.n_neighbor, self.dim)),\n",
    "                    neighbor_relations=relation_vectors[hop] \\\n",
    "                        .view((self.batch_size, -1, self.n_neighbor, self.dim)),\n",
    "                    user_embeddings=user_embeddings,\n",
    "                    act=act\n",
    "                )\n",
    "                entity_vectors_next_iter.append(vector)\n",
    "\n",
    "            # 다음 hop의 결과로 갱신\n",
    "            entity_vectors = entity_vectors_next_iter\n",
    "\n",
    "        # 최종 item 임베딩 반환\n",
    "        return entity_vectors[0].view((self.batch_size, self.dim))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
